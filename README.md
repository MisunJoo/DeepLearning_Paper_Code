# DeepLearning Paper with Code 
## 스터디 개요
-   스터디 목표
    -   주요 딥러닝 논문을 보며 모델링을 할 때 어떻게 코드로 작성할 수 있는지를 학습
-   스터디 방법
    -   **매주 화요일**에 모여 돌아가면서 그 주의 논문을 읽고 코드로 구현하고 와서 각자 맡은 파트를 스터디 원에게 설명

## 커리큘럼
|              <div style="width:50px">주차</div> |논문                          |        <div style="width:50px">발표자 </div>             			|
|----------------|-------------------------------|-----------------------------|
|1주차 | AlexNet - [ImageNet Classification with Deep Convolutional Neural Network](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)        |조종호|
|1주차 |GoogLeNet - [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)|주미선 |
|1주차 |VGG - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)|박소민|				
|2주차 |ResNet - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)        |조종호|
|3주차 |UNet - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)|주미선 |
|4주차 |Inception v4 - [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)|박소민|				
|5주차 |DenseNet - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)|조종호|
|6주차 | PreAct-ResNet - [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)|주미선 |
|6주차 | WRN- [Wide Residual Networks](https://arxiv.org/pdf/1605.07146.pdf)|박소민 |
|7주차 | Residual Attention Network - [Residual Attention Network for Image Classification](https://arxiv.org/pdf/1704.06904.pdf)|조종호 |
|7주차 | Xception - [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/pdf/1610.02357.pdf)|주미선 |
|8주차 | SENet - [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)|박소민 |
|8주차 | MobileNet - [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)|조종호 |
|9주차 | CAM - [Learning Deep Features for Discriminative Localization](https://arxiv.org/pdf/1512.04150.pdf)|주미선 |
|9주차 | EfficientNet - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf)|박소민 |
|10주차 | Transformer - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)|조종호 |
|10주차 | ViT - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf2)|주미선 |